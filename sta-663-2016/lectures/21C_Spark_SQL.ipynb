{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL\n",
    "====\n",
    "\n",
    "- [Official Documentation](http://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "\n",
    "A tour of the Spark SQL library, the `spark-csv` package and Spark DataFrmaes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "-----\n",
    "\n",
    "- [Spark tutoials](http://www.sparktutorials.net/tutorials): A growing bunch of accessible tutorials on Spark, mostly in Scala but a few in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "conf = (SparkConf()\n",
    "        .setAppName('SparkSQL')\n",
    "        .setMaster('local[*]'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlc = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame from `pandas`\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pandas_df = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spark_df = sqlc.createDataFrame(pandas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|species|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|        0.2| setosa|\n",
      "|         4.9|        3.0|         1.4|        0.2| setosa|\n",
      "|         4.7|        3.2|         1.3|        0.2| setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame from CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using manual parsing and a schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "year,make,model,comment,blank\n",
      "\"2012\",\"Tesla\",\"S\",\"No comment\",\n",
      "\n",
      "1997,Ford,E350,\"Go get one now they are going fast\",\n",
      "2015,Chevy,Volt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cat data/cars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+--------------------+-----+\n",
      "|year|   make|model|             comment|blank|\n",
      "+----+-------+-----+--------------------+-----+\n",
      "|null|\"Tesla\"|  \"S\"|        \"No comment\"|     |\n",
      "|null|   Ford| E350|\"Go get one now t...|     |\n",
      "|null|  Chevy| Volt|                    |     |\n",
      "+----+-------+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "def pad(alist):\n",
    "    tmp = alist[:]\n",
    "    n = 5 - len(alist)\n",
    "    for i in range(n):\n",
    "        tmp.append('')\n",
    "    return tmp\n",
    "\n",
    "# Load a text file and convert each line to a tuple.\n",
    "lines = sc.textFile('data/cars.csv')\n",
    "header = lines.first() #extract header\n",
    "lines = lines.filter(lambda line: line != header)\n",
    "lines = lines.filter(lambda line: line)\n",
    "parts = lines.map(lambda l: l.split(','))\n",
    "parts = parts.map(lambda part: pad(part))\n",
    "\n",
    "fields = [    \n",
    "    StructField('year', IntegerType(), True),\n",
    "    StructField('make', StringType(), True),\n",
    "    StructField('model', StringType(), True),\n",
    "    StructField('comment', StringType(), True),\n",
    "    StructField('blank', StringType(), True),\n",
    "    ]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# Apply the schema to the RDD.\n",
    "df0 = sqlc.createDataFrame(parts, schema)\n",
    "\n",
    "df0.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the `spark-csv` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = (sqlc.read.format('com.databricks.spark.csv')\n",
    "      .options(header='true', inferschema='true')\n",
    "      .load('data/cars.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the datframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: integer (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- comment: string (nullable = true)\n",
      " |-- blank: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+--------------------+-----+\n",
      "|year| make|model|             comment|blank|\n",
      "+----+-----+-----+--------------------+-----+\n",
      "|2012|Tesla|    S|          No comment|     |\n",
      "|1997| Ford| E350|Go get one now th...|     |\n",
      "|2015|Chevy| Volt|                null| null|\n",
      "+----+-----+-----+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year| make|\n",
      "+----+-----+\n",
      "|2012|Tesla|\n",
      "|1997| Ford|\n",
      "|2015|Chevy|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['year', 'make']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run SQL queries, we need to register the dataframe as a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('cars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|year| make|\n",
      "+----+-----+\n",
      "|2012|Tesla|\n",
      "|2015|Chevy|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = sqlc.sql('select year, make from cars where year > 2000')\n",
    "q.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark dataframes can be converted to Pandas ones\n",
    "\n",
    "Typically, we would only convert small dataframes such as the results of SQL queries. If we could load the original dataset in memory as a `pandaa` dataframe, why would we be using Spark?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>make</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012</td>\n",
       "      <td>Tesla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015</td>\n",
       "      <td>Chevy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year   make\n",
       "0  2012  Tesla\n",
       "1  2015  Chevy"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_df = q.toPandas()\n",
    "q_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "DataFrame from JSON files\n",
    "----\n",
    "\n",
    "It is easier to read in JSON than CSV files because JSON is self-describing, allowing Spark SQL to infer the appropriate schema without additional hints.\n",
    "\n",
    "As an example, we will look at Durham police crime reports from the [Dhrahm Open Data](https://opendurham.nc.gov/page/home/) website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlc.read.json('data/durham-police-crime-reports.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many records are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101375"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is JSON, it is possible to have a nested schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- datasetid: string (nullable = true)\n",
      " |-- fields: struct (nullable = true)\n",
      " |    |-- addtime: string (nullable = true)\n",
      " |    |-- big_zone: string (nullable = true)\n",
      " |    |-- chrgdesc: string (nullable = true)\n",
      " |    |-- csstatus: string (nullable = true)\n",
      " |    |-- csstatusdt: string (nullable = true)\n",
      " |    |-- date_fnd: string (nullable = true)\n",
      " |    |-- date_occu: string (nullable = true)\n",
      " |    |-- date_rept: string (nullable = true)\n",
      " |    |-- dist: string (nullable = true)\n",
      " |    |-- dow1: string (nullable = true)\n",
      " |    |-- dow2: string (nullable = true)\n",
      " |    |-- geo_point_2d: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- geo_shape: struct (nullable = true)\n",
      " |    |    |-- coordinates: array (nullable = true)\n",
      " |    |    |    |-- element: double (containsNull = true)\n",
      " |    |    |-- type: string (nullable = true)\n",
      " |    |-- hour_fnd: string (nullable = true)\n",
      " |    |-- hour_occu: string (nullable = true)\n",
      " |    |-- hour_rept: string (nullable = true)\n",
      " |    |-- inci_id: string (nullable = true)\n",
      " |    |-- monthstamp: string (nullable = true)\n",
      " |    |-- reportedas: string (nullable = true)\n",
      " |    |-- reviewdate: string (nullable = true)\n",
      " |    |-- strdate: string (nullable = true)\n",
      " |    |-- ucr_code: string (nullable = true)\n",
      " |    |-- ucr_type_o: string (nullable = true)\n",
      " |    |-- yearstamp: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: double (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- record_timestamp: string (nullable = true)\n",
      " |-- recordid: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show the top few rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|           datasetid|              fields|            geometry|    record_timestamp|            recordid|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|durham-police-cri...|[2013-12-01T19:00...|[WrappedArray(-78...|2016-03-12T02:32:...|2c0251654c4b7a006...|\n",
      "|durham-police-cri...|[2013-12-01T19:00...|[WrappedArray(-78...|2016-03-12T02:32:...|e5fe0e483fdb17fb7...|\n",
      "|durham-police-cri...|[2013-12-01T19:00...|[WrappedArray(-78...|2016-03-12T02:32:...|d16c330ea4b3e2a90...|\n",
      "|durham-police-cri...|[2013-12-01T19:00...|[WrappedArray(-78...|2016-03-12T02:32:...|1128e12a912b16cfe...|\n",
      "|durham-police-cri...|[2013-12-01T19:00...|[WrappedArray(-78...|2016-03-12T02:32:...|ac79bc9c709d5dfa4...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a dataframe only containing date and charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|    strdate|            chrgdesc|\n",
      "+-----------+--------------------+\n",
      "|Dec  2 2013|CALLS FOR SERVICE...|\n",
      "|Dec  2 2013|VANDALISM TO PROP...|\n",
      "|Dec  2 2013|BURGLARY - FORCIB...|\n",
      "|Dec  2 2013|LARCENY - SHOPLIF...|\n",
      "|Dec  2 2013|BURGLARY - FORCIB...|\n",
      "+-----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(['fields.strdate', 'fields.chrgdesc']).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show distinct charges - note that for an actual analysis, you would probably want to consolidate these into a smaller number of groups to account for typos, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            chrgdesc|\n",
      "+--------------------+\n",
      "|ALL OTHER OFFENSE...|\n",
      "|DRUG EQUIPMENT/PA...|\n",
      "| ASSIST OTHER AGENCY|\n",
      "|TOWED/ABANDONED V...|\n",
      "|DRUG EQUIPMENT/PA...|\n",
      "|BURGLARY - FORCIB...|\n",
      "|SEX OFFENSE - STA...|\n",
      "|ROBBERY - INDIVIDUAL|\n",
      "|WEAPON VIOLATIONS...|\n",
      "|ALL OTHER OFFENSE...|\n",
      "|DRUG/NARCOTIC VIO...|\n",
      "|SEX OFFENSE - PEE...|\n",
      "|DRUG/NARCOTIC VIO...|\n",
      "|DRUG/NARCOTIC VIO...|\n",
      "|AGGRAVATED ASSAUL...|\n",
      "|ALL OTHER OFFENSE...|\n",
      "|LIQUOR LAW - POSS...|\n",
      "|EMBEZZLEMENT - WI...|\n",
      "|WEAPON VIOLATIONS...|\n",
      "|             RUNAWAY|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select('fields.chrgdesc').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What charges are the most common?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            chrgdesc|count|\n",
      "+--------------------+-----+\n",
      "|BURGLARY - FORCIB...|11630|\n",
      "|LARCENY - SHOPLIF...| 7633|\n",
      "|LARCENY - FROM MO...| 7405|\n",
      "|SIMPLE ASSAULT (P...| 5085|\n",
      "| LARCENY - ALL OTHER| 4666|\n",
      "|LARCENY - FROM BU...| 4514|\n",
      "|VANDALISM TO AUTO...| 4112|\n",
      "|DRUG/NARCOTIC VIO...| 3790|\n",
      "|LARCENY - AUTOMOB...| 3441|\n",
      "|VANDALISM TO PROP...| 3422|\n",
      "|CALLS FOR SERVICE...| 3207|\n",
      "|  AGGRAVATED ASSAULT| 3183|\n",
      "|BURGLARY - NON-FO...| 2339|\n",
      "|ROBBERY - INDIVIDUAL| 2330|\n",
      "|TOWED/ABANDONED V...| 2244|\n",
      "|MOTOR VEHICLE THE...| 1970|\n",
      "|DRIVING WHILE IMP...| 1912|\n",
      "|FRAUD - FALSE PRE...| 1660|\n",
      "|      FOUND PROPERTY| 1643|\n",
      "|ALL TRAFFIC (EXCE...| 1436|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupby('fields.chrgdesc').count().sort('count', ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register as table to run full SQL queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.registerTempTable('crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            chrgdesc|count|\n",
      "+--------------------+-----+\n",
      "|ALL OTHER OFFENSE...|    1|\n",
      "|TOWED/ABANDONED V...|  258|\n",
      "| ASSIST OTHER AGENCY|   19|\n",
      "|BURGLARY - FORCIB...|  929|\n",
      "|SEX OFFENSE - STA...|    3|\n",
      "|ROBBERY - INDIVIDUAL|  157|\n",
      "|WEAPON VIOLATIONS...|    6|\n",
      "|SEX OFFENSE - PEE...|    5|\n",
      "|ALL OTHER OFFENSE...|    8|\n",
      "|DRUG/NARCOTIC VIO...|   14|\n",
      "|DRUG/NARCOTIC VIO...|   28|\n",
      "|AGGRAVATED ASSAUL...|    1|\n",
      "|LIQUOR LAW - POSS...|    2|\n",
      "|ALL OTHER OFFENSE...|    3|\n",
      "|EMBEZZLEMENT - WI...|    7|\n",
      "|WEAPON VIOLATIONS...|    1|\n",
      "|             RUNAWAY|   87|\n",
      "|      MISSING PERSON|   16|\n",
      "|SIMPLE ASSAULT-PH...|    3|\n",
      "|ALL OTHER OFFENSE...|   22|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = sqlc.sql('''\n",
    "select fields.chrgdesc, count(fields.chrgdesc) as count\n",
    "from crimes \n",
    "where fields.monthstamp=3\n",
    "group by fields.chrgdesc\n",
    "''')\n",
    "q.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chrgdesc</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ALL OTHER OFFENSES-BIGAMY/MARRIAGE LAWS</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TOWED/ABANDONED VEHICLE</td>\n",
       "      <td>258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASSIST OTHER AGENCY</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BURGLARY - FORCIBLE ENTRY</td>\n",
       "      <td>929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SEX OFFENSE - STATUTORY RAPE</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  chrgdesc  count\n",
       "0  ALL OTHER OFFENSES-BIGAMY/MARRIAGE LAWS      1\n",
       "1                  TOWED/ABANDONED VEHICLE    258\n",
       "2                      ASSIST OTHER AGENCY     19\n",
       "3                BURGLARY - FORCIBLE ENTRY    929\n",
       "4             SEX OFFENSE - STATUTORY RAPE      3"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crimes_df = q.toPandas()\n",
    "crimes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrame from SQLite3\n",
    "----\n",
    "\n",
    "The official docs suggest that this can be done directly via JDBC but I cannot get it to work. As a workaround, you can convert to JSON before importing as a dataframe. If anyone finds out how to load an SQLite3 database table directly into a Spark datafraeme, please let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+--------------------+\n",
      "|AlbumId|ArtistId|               Title|\n",
      "+-------+--------+--------------------+\n",
      "|      1|       1|For Those About T...|\n",
      "|      2|       2|   Balls to the Wall|\n",
      "|      3|       2|   Restless and Wild|\n",
      "+-------+--------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from odo import odo\n",
    "\n",
    "odo('sqlite:///../data/Chinook_Sqlite.sqlite::Album', 'Album.json')\n",
    "df = sqlc.read.json('Album.json')\n",
    "df.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataSets\n",
    "----\n",
    "\n",
    "In Scala and Java, Spark 1.6 introduced a new type called `DataSet` that combines the relational properties of a `DataFrame` with the functional methods of an `RDD`. This will be available in Python in a later version. However, because of the dynamic nature of Python, you can already call functional methods on a Spark `Dataframe`, giving most of the ease of use of the `DataSet` type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = sqlc.read.text('../data/Ulysses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[value: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|The Project Guten...|\n",
      "|                    |\n",
      "|This eBook is for...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ds.show(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    import string\n",
    "    return s.translate(dict.fromkeys(ord(c) for c in string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = (ds.map(lambda x: remove_punctuation(x[0]))\n",
    "            .flatMap(lambda x: x.lower().strip().split())\n",
    "            .filter(lambda x:  x!= '')\n",
    "            .map(lambda x: (x, 1))\n",
    "            .countByKey())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 15107),\n",
       " ('of', 8257),\n",
       " ('and', 7282),\n",
       " ('a', 6553),\n",
       " ('to', 5042),\n",
       " ('in', 4981),\n",
       " ('he', 4033),\n",
       " ('his', 3333),\n",
       " ('i', 2698),\n",
       " ('that', 2621)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(counts.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional Exercise**\n",
    "\n",
    "The crime data set includes both date and geospatial information. Consider creating an interactive map visualization of crimes in Durham by date using the `bokeh` package. See this [example](http://bokeh.pydata.org/en/0.11.1/docs/user_guide/geo.html) to get started. GeoJSON version of the Durham Police Crime Reports can be [downloaded](https://opendurham.nc.gov/explore/dataset/durham-police-crime-reports/download/?format=geojson&timezone=America/New_York)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.5.1 64bit [GCC 4.2.1 (Apple Inc. build 5577)]"
        },
        {
         "module": "IPython",
         "version": "4.0.3"
        },
        {
         "module": "OS",
         "version": "Darwin 15.4.0 x86_64 i386 64bit"
        },
        {
         "module": "pyspark",
         "version": "The 'pyspark' distribution was not found and is required by the application"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.5.1 64bit [GCC 4.2.1 (Apple Inc. build 5577)]</td></tr><tr><td>IPython</td><td>4.0.3</td></tr><tr><td>OS</td><td>Darwin 15.4.0 x86_64 i386 64bit</td></tr><tr><td>pyspark</td><td>The 'pyspark' distribution was not found and is required by the application</td></tr><tr><td colspan='2'>Wed Apr 20 11:54:43 2016 EDT</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.5.1 64bit [GCC 4.2.1 (Apple Inc. build 5577)] \\\\ \\hline\n",
       "IPython & 4.0.3 \\\\ \\hline\n",
       "OS & Darwin 15.4.0 x86\\_64 i386 64bit \\\\ \\hline\n",
       "pyspark & The 'pyspark' distribution was not found and is required by the application \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Wed Apr 20 11:54:43 2016 EDT} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.5.1 64bit [GCC 4.2.1 (Apple Inc. build 5577)]\n",
       "IPython 4.0.3\n",
       "OS Darwin 15.4.0 x86_64 i386 64bit\n",
       "pyspark The 'pyspark' distribution was not found and is required by the application\n",
       "Wed Apr 20 11:54:43 2016 EDT"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

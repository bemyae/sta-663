{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Spark 02\n",
    "====\n",
    "\n",
    "Focus in this lecture is on Spark constructs that can make your programs more efficient. In general, this means minimizing the amount of data transfer across nodes, since this is usually the bottleneck for big data analysis problems.\n",
    "\n",
    "- Shared variables\n",
    "    - Accumulators\n",
    "    - Broadcast variables\n",
    "- DataFrames\n",
    "- Partitioning and the Spark shuffle\n",
    "- Piping to external programs\n",
    "\n",
    "Spark tuning and optimization is complicated - this tutorial only touches on some of the basic concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext('local[*]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resources\n",
    "----\n",
    "\n",
    "[The Spark Programming Guide](http://spark.apache.org/docs/latest/programming-guide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accumulators\n",
    "----\n",
    "\n",
    "Spark functions such as `map` can use variables defined in the driver program, but they make local copies of the variable that are not passed back to the driver program. Accumulators are *shared variable* that allow the aggregation of results from workers back to the driver program, for example, as an event counter. Suppose we want to count the number of rows of data with missing information. The most efficient way is to use an **accumulator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ulysses = sc.textFile('data/Ulysses.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of Ulysses, by James Joyce',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included',\n",
       " 'with this eBook or online at www.gutenberg.org',\n",
       " '',\n",
       " '',\n",
       " 'Title: Ulysses',\n",
       " '']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulysses.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Event counting\n",
    "\n",
    "Notice that we have some empty lines. We want to count the number of non-empty lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_lines = sc.accumulator(0)\n",
    "\n",
    "def tokenize(line):\n",
    "    table = dict.fromkeys(map(ord, string.punctuation))\n",
    "    return line.translate(table).lower().strip().split()\n",
    "\n",
    "def tokenize_count(line):\n",
    "    global num_lines\n",
    "    \n",
    "    if line:\n",
    "        num_lines += 1\n",
    "\n",
    "    return tokenize(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counter = ulysses.flatMap(lambda line: tokenize_count(line)).countByValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter['circle']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25396"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcast Variables\n",
    "----\n",
    "\n",
    "Sometimes we need to send a large read only variable to all workers. For example, we might want to share a large feature matrix to all workers as a part of a machine learning application. This same variable will be sent separately for each parallel operation unless you use a **broadcast variable**. Also, the default variable passing mechanism is optimized for small variables and can be slow when the variable is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "table = dict(zip(string.ascii_letters, count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weight_first(line, table):\n",
    "    words = tokenize(line)\n",
    "    return sum(table.get(word[0], 0) for word in words if word.isalpha())\n",
    "\n",
    "def weight_last(line, table):\n",
    "    words = tokenize(line)\n",
    "    return sum(table.get(word[-1], 0) for word in words if word.isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The dictionary `table` is sent out twice to worker nodes, one for each call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2941855"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulysses.map(lambda line: weight_first(line, table)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995994"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulysses.map(lambda line: weight_last(line, table)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting to use broadast variables is simple and more efficient\n",
    "\n",
    "- Use SparkContext.broadcast() to create a broadcast variable\n",
    "- Where you would use var, use var.value\n",
    "- The broadcast variabel is sent once to each node and can be re-used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "table_bc = sc.broadcast(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_first_bc(line, table):\n",
    "    words = tokenize(line)\n",
    "    return sum(table.value.get(word[0], 0) for word in words if word.isalpha())\n",
    "\n",
    "def weight_last_bc(line, table):\n",
    "    words = tokenize(line)\n",
    "    return sum(table.value.get(word[-1], 0) for word in words if word.isalpha())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### table_bc is sent to nodes only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2941855"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulysses.map(lambda line: weight_first_bc(line, table_bc)).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2995994"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ulysses.map(lambda line: weight_last_bc(line, table_bc)).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataFrames\n",
    "----\n",
    "\n",
    "Spark has a DataFrame data structure that provides an interface for working with mixed type data sets, greatly simplifying distributed statistical data processing. DataFrames also allow easy conversion and merging of data from different sources (e.g. distributed databases or filesystem storage formats) and are highly optimized.\n",
    "\n",
    "**Reminder** As usual - if your data set fits into memory, just use `pandas` - Spark is unlikely to provide any performance benefits unless your data set is truly massive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sql = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/titanic.csv'\n",
    "titanic = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[survived: bigint, pclass: bigint, sex: string, age: double, sibsp: bigint, parch: bigint, fare: double, embarked: string, class: string, who: string, adult_male: boolean, deck: double, embark_town: string, alive: string, alone: boolean]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sql.createDataFrame(titanic)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(survived=0, pclass=3, sex='male', age=22.0, sibsp=1, parch=0, fare=7.25, embarked='S', class='Third', who='man', adult_male=True, deck=nan, embark_town='Southampton', alive='no', alone=False)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functional programming constructs still work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Row(embark_town='Queenstown'), 77),\n",
       " (Row(embark_town='NaN'), 2),\n",
       " (Row(embark_town='Southampton'), 644),\n",
       " (Row(embark_town='Cherbourg'), 168)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(\n",
    "df.select('embark_town')\n",
    ".map(lambda town: (town, 1))\n",
    ".reduceByKey(lambda x, y: x+y)\n",
    ".collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can use familiar methods like `groupby`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "counts = df.groupby(['sex', 'class']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(sex='female', class='First', count=94),\n",
       " Row(sex='female', class='Third', count=144),\n",
       " Row(sex='male', class='Second', count=108),\n",
       " Row(sex='female', class='Second', count=76),\n",
       " Row(sex='male', class='First', count=122),\n",
       " Row(sex='male', class='Third', count=347)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can convert from Spark to Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sex</th>\n",
       "      <th>class</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>First</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female</td>\n",
       "      <td>Third</td>\n",
       "      <td>144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>male</td>\n",
       "      <td>Second</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female</td>\n",
       "      <td>Second</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>male</td>\n",
       "      <td>First</td>\n",
       "      <td>122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male</td>\n",
       "      <td>Third</td>\n",
       "      <td>347</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      sex   class  count\n",
       "0  female   First     94\n",
       "1  female   Third    144\n",
       "2    male  Second    108\n",
       "3  female  Second     76\n",
       "4    male   First    122\n",
       "5    male   Third    347"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark Shuffle and Partitioning\n",
    "----\n",
    "\n",
    "Some events trigger the redistribution of data across partitions, and involves the (expensive) copying of data across executors and machines. This is known as the **shuffle**. For example, if we do a `reduceByKey` operation on key-value pair RDD, Spark needs to collect all pairs with the same key in the same partition to do the reduction. \n",
    "\n",
    "For key-value RDDs, you have some control over the partitioning of the RDDs. In particular, you can ask Spark to partition a set of keys so that they are guaranteed to appear together on some node. This can minimize a lot of data transfer. For example, suppose you have a large key-value RDD consisting of user_name: comments from a web user community. Every night, you want to update with new user comments with a join operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fake_data(n, val):\n",
    "    users = list(map(''.join, np.random.choice(list(string.ascii_lowercase), (n,2))))\n",
    "    comments = [val]*n\n",
    "    return tuple(zip(users, comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nk', 0),\n",
       " ('pq', 0),\n",
       " ('ly', 0),\n",
       " ('pq', 0),\n",
       " ('oo', 0),\n",
       " ('lw', 0),\n",
       " ('cw', 0),\n",
       " ('au', 0),\n",
       " ('ce', 0),\n",
       " ('pt', 0)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fake_data(10000, 0)\n",
    "list(data)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(data).reduceByKey(lambda x, y: x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xf', 1),\n",
       " ('xz', 1),\n",
       " ('ke', 1),\n",
       " ('au', 1),\n",
       " ('uq', 1),\n",
       " ('vi', 1),\n",
       " ('lu', 1),\n",
       " ('pv', 1),\n",
       " ('nc', 1),\n",
       " ('xt', 1)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = fake_data(100, 1)\n",
    "list(new_data)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_new = sc.parallelize(new_data).reduceByKey(lambda x, y: x+y).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd_updated = rdd.join(rdd_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xf', (0, 1)),\n",
       " ('rx', (0, 1)),\n",
       " ('wf', (0, 1)),\n",
       " ('vo', (0, 1)),\n",
       " ('cb', (0, 2)),\n",
       " ('au', (0, 1)),\n",
       " ('hp', (0, 1)),\n",
       " ('ar', (0, 2)),\n",
       " ('oq', (0, 1)),\n",
       " ('gf', (0, 1))]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_updated.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Using `partitionBy`\n",
    "\n",
    "The `join` operation will hash all the keys of both `rdd` and `rdd_nerw`, sending keys with the same hashes to the same node for the actual join operation. There is a lot of unnecessary data transfer. Since `rdd` is a much larger data set than `rdd_new`, we can instead fix the partitioning of `rdd` and just transfer the keys of `rdd_new`. This is done by `rdd.partitionBy(numPartitions)` where `numPartitions` should be at least twice the number of cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2 = sc.parallelize(data).reduceByKey(lambda x, y: x+y)\n",
    "rdd2 = rdd2.partitionBy(10).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd2_updated = rdd2.join(rdd_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('zs', (0, 1)),\n",
       " ('hp', (0, 1)),\n",
       " ('wq', (0, 1)),\n",
       " ('vf', (0, 1)),\n",
       " ('xf', (0, 1)),\n",
       " ('rd', (0, 1)),\n",
       " ('nc', (0, 1)),\n",
       " ('dy', (0, 1)),\n",
       " ('dz', (0, 1)),\n",
       " ('zl', (0, 1))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2_updated.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Piping to External Programs\n",
    "----\n",
    "\n",
    "Suppose it is more convenient or efficient to write a function in some other language to process data. We can **pipe** data from Spark to the external program (script) that performs the calculation via standard input and output. The example below shows using a C++ program to calculate the sum of squares for collections of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting foo.cpp\n"
     ]
    }
   ],
   "source": [
    "%%file foo.cpp\n",
    "\n",
    "#include <iostream>\n",
    "#include <sstream>\n",
    "#include <string>\n",
    "#include <numeric>\n",
    "#include <vector>\n",
    "using namespace std;\n",
    "\n",
    "double sum_squares(double x, double y) {\n",
    "  return x + y*y;\n",
    "};\n",
    "\n",
    "int main() {\n",
    "\n",
    "    string s;\n",
    "\n",
    "    while (cin) {\n",
    "\n",
    "        getline(cin, s);\n",
    "        stringstream stream(s);\n",
    "        vector<double> v;\n",
    "\n",
    "        while(1) {\n",
    "            double u;\n",
    "            stream >> u;\n",
    "            if(!stream)\n",
    "                break;\n",
    "            v.push_back(u);\n",
    "        }\n",
    "        double x = accumulate(v.begin(), v.end(), 0.0, sum_squares);\n",
    "        cout << x << endl;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! g++ foo.cpp -o foo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xs = np.random.random((100, 3))\n",
    "np.savetxt('numbers.txt', xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.007290252044240164e-01 9.388840172344149471e-01 7.641926913121750431e-01\r\n",
      "4.797444598292510687e-02 6.371952930746385135e-01 1.728628973849547501e-01\r\n",
      "7.368128692587038175e-01 9.560134392124460812e-01 1.520119012528126090e-01\r\n",
      "6.138786203057124968e-01 2.304246773308167295e-01 7.612600522864221286e-01\r\n",
      "5.858370203064489079e-02 9.319799159225684582e-01 8.741906149418643412e-02\r\n",
      "5.974674822676994124e-01 2.354333172028766352e-01 2.251495926817642657e-01\r\n",
      "5.743565883133471273e-01 7.083800554857178078e-01 4.511509816992009814e-01\r\n",
      "1.626631002782216173e-01 1.591666834563804089e-01 3.510692468417206946e-01\r\n",
      "6.374886282436751372e-01 5.362041285754336473e-01 6.951278106461864503e-01\r\n",
      "9.591589607284622954e-01 7.159018076623913096e-02 4.989118457521155570e-01\r\n"
     ]
    }
   ],
   "source": [
    "!head numbers.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rdd = sc.textFile('numbers.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkFiles\n",
    "\n",
    "def prepare(line):\n",
    "    \"\"\"Each line contains numbers separated by a space.\"\"\"\n",
    "    return ' '.join(line.split()) + '\\n'\n",
    "\n",
    "# pipe data to external function\n",
    "func = './foo'\n",
    "sc.addFile(func)\n",
    "ss = rdd.map(lambda s: prepare(s)).pipe(SparkFiles.get(func))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2.10666', '0.438201', '1.47996']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.10666064,  0.43820097,  1.47996252])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(xs[:3]**2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "3.5.1 64bit [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]"
        },
        {
         "module": "IPython",
         "version": "4.0.1"
        },
        {
         "module": "OS",
         "version": "Linux 4.2.0 23 generic x86_64 with debian jessie sid"
        },
        {
         "module": "pyspark",
         "version": "The 'pyspark' distribution was not found and is required by the application"
        },
        {
         "module": "numpy",
         "version": "1.10.2"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>3.5.1 64bit [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]</td></tr><tr><td>IPython</td><td>4.0.1</td></tr><tr><td>OS</td><td>Linux 4.2.0 23 generic x86_64 with debian jessie sid</td></tr><tr><td>pyspark</td><td>The 'pyspark' distribution was not found and is required by the application</td></tr><tr><td>numpy</td><td>1.10.2</td></tr><tr><td colspan='2'>Mon Jan 18 19:44:12 2016 EST</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 3.5.1 64bit [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] \\\\ \\hline\n",
       "IPython & 4.0.1 \\\\ \\hline\n",
       "OS & Linux 4.2.0 23 generic x86\\_64 with debian jessie sid \\\\ \\hline\n",
       "pyspark & The 'pyspark' distribution was not found and is required by the application \\\\ \\hline\n",
       "numpy & 1.10.2 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Mon Jan 18 19:44:12 2016 EST} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 3.5.1 64bit [GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
       "IPython 4.0.1\n",
       "OS Linux 4.2.0 23 generic x86_64 with debian jessie sid\n",
       "pyspark The 'pyspark' distribution was not found and is required by the application\n",
       "numpy 1.10.2\n",
       "Mon Jan 18 19:44:12 2016 EST"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext version_information\n",
    "%version_information pyspark, numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

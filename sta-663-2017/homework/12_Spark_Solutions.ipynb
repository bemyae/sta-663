{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>0</td><td></td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkContext available as 'sc'.\n",
      "HiveContext available as 'sqlContext'.\n"
     ]
    }
   ],
   "source": [
    "%%spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/usr/bin/python3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note 1**: In the following exercises, keep the amount of data returned by Spark to the local notebook session to the minimum needed for that exercise. In other words, all the work should be done via distributed computing and not by returning a large collection that is then processed in regular Python.\n",
    "\n",
    "**Note 2**: To minimize waitinng times, do the exercises using the C. elegans genome. Human genome is at `/data/human/*fa` but takes a long time to processs, so you don't have to analyze the human genome unless you want to see how long it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change path when debugging is compete to work on human genome\n",
    "\n",
    "# fasta_path = '/data/human/*fa'\n",
    "fasta_path = '/data/c_elegans/*fa'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 (50 points)**\n",
    "\n",
    "Write a program using `spark` to find 5 most common k-mers (shifting windows of length k) in the human genome. Ignore case when processing k-mers. You can work one line at a time - we will ignore k-mers that wrap around lines. You should write a function that takes a path to FASTA files and a value for k, and returns an key-value RDD of k-mer counts. Remember to strip comment lines that begin with '>' from the anlaysis. \n",
    "\n",
    "**Note**: The textFile method takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Please set this paramter to 60 - it will speed up processing.\n",
    "\n",
    "**Check**: Use the C. elegans genome at `/data/c_elegans/*fa`. You should get \n",
    "\n",
    "```\n",
    "[\n",
    "(u'ATATATATATATATATATAT', 2168), \n",
    "(u'TATATATATATATATATATA', 2142), \n",
    "(u'CTCTCTCTCTCTCTCTCTCT', 1337), \n",
    "(u'TCTCTCTCTCTCTCTCTCTC', 1327), \n",
    "(u'AGAGAGAGAGAGAGAGAGAG', 1007)\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_kmers(path, k):\n",
    "    \"\"\"Returns RDD of coutns of k-mers for FASTA files in path.\"\"\"\n",
    "    \n",
    "    counts = (sc.textFile(fasta_path, 60)\n",
    "              .map(lambda line: line.upper())\n",
    "              .filter(lambda line: line.strip() and not line.startswith('>') )\n",
    "              .flatMap(lambda read: [read[i:i+k] for i in range(len(read) - k + 1)])\n",
    "              .map(lambda kmer: (kmer, 1))\n",
    "              .reduceByKey(lambda a, b: a + b))\n",
    "    \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'ATATATATATATATATATAT', 2217), (u'TATATATATATATATATATA', 2184), (u'CTCTCTCTCTCTCTCTCTCT', 1373), (u'TCTCTCTCTCTCTCTCTCTC', 1361), (u'AGAGAGAGAGAGAGAGAGAG', 1033)]"
     ]
    }
   ],
   "source": [
    "k = 20\n",
    "counts = count_kmers(fasta_path, k)\n",
    "counts.takeOrdered(5, key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2 (10 points)**\n",
    "\n",
    "Find all k-mers that are palindromes. How many are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_palindrome(seq):\n",
    "    return seq == seq[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting unique palindromes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "951"
     ]
    }
   ],
   "source": [
    "palindromes = counts.filter(lambda x: is_palindrome(x[0]))\n",
    "palindromes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting all palindromes including repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2816"
     ]
    }
   ],
   "source": [
    "palindromes.map(lambda x: x[1]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 (10 points)** \n",
    "\n",
    "As a simple QC measure, we can assume that the k-mers that have a count of only 1 are due to sequencing errors. Put all the k-mers with a count of 2 or more in a Spark DataFrame with two columns (sequence, count). Count how many rows in the DataFrame have counts between 5 and 10 (inclusive of both 5 and 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = counts.filter(lambda x: x[1] > 1).toDF(['seqeunce', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174486"
     ]
    }
   ],
   "source": [
    "df.filter((df['count'] >= 5) & (df['count'] <= 10)).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercsie 4 (30 points)**\n",
    "\n",
    "Make a Markov transition matrix for any nucleotide ('A', 'C', 'T', 'G') to any other nucleotide. The (i,j) entry should indicate the probability of finding the jth nucleotide appearing immediaely after the ith nucleotide in the genome. For example, the entry (0, 2) shows the probability of finding a T immediately followng an A. The matrix should have shape (4,4). Ignore any letter not in 'ACTG'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pairs = count_kmers(fasta_path, 2).collectAsMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[ 0.41930811,  0.14972777,  0.27417737,  0.15678674],\n",
      "       [ 0.34893751,  0.18929553,  0.28566181,  0.17610515],\n",
      "       [ 0.196519  ,  0.19243038,  0.41931672,  0.19173389],\n",
      "       [ 0.35093673,  0.18814073,  0.27257127,  0.18835127]])"
     ]
    }
   ],
   "source": [
    "def mapper(s):\n",
    "    nuc = 'ACTG'\n",
    "    if not s[0] in nuc or not s[1] in nuc:\n",
    "        return None\n",
    "    i, j = nuc.index(s[0]), nuc.index(s[1])\n",
    "    return i, j\n",
    "\n",
    "M = np.zeros((4,4))\n",
    "for key in pairs:\n",
    "    if mapper(key) is None:\n",
    "        continue\n",
    "    i, j = mapper(key)\n",
    "    M[i, j] = pairs[key]\n",
    "M/M.sum(axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

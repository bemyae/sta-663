{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**. The most common parallelization task is to run the same function on different sets of data.  For example, we may need to read in many files, do some processing on each file, and finally merge the results.\n",
    "\n",
    "- Write a function to perform a word count given an iterable of strings. The function strips leading and trailing spaces from each string, removes punctuation, converts to lower case and splits by blank space. It returns a dictionary whose keys are words and whose values are the counts of that word. For example\n",
    "\n",
    "```\n",
    "word_count([\"This is a string.\", \"This is another string!\"])\n",
    "```\n",
    "\n",
    "returns \n",
    "\n",
    "```\n",
    "{'a': 1, 'another': 1, 'is': 2, 'string': 2, 'this': 2}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**. We first need to download the text files to run `word_count` on. This is network rather than CPU bound, so threading or asynchronous calls work well here.  Read a list of names of files that can be found at `http://http://people.duke.edu/~ccc14/jokes`. \n",
    "\n",
    "Time how long it takes to download all named files using the `%%time` cell magic\n",
    "- Use a for loop  \n",
    "- Use a `ThreadPoolExecutor` from `concurrent.futures` \n",
    "- Using asynchronous calls and the `asyncio` event loop\n",
    "\n",
    "To download a file, first save the response from `requests.get(url)` as a variable `r`. Then write `r.text` into a file using standard Python syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**. Now use processes from either `concurrent.futures` or `multiprocessing` to parallelize the counting of words in each downloaded file. Finally, merge the returned dictionaries and print the 10 most common words and their counts from all files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**. Recall the `cdist` function from the previus worksheet. Suppose we had a large number of vectors to work with. Parallelize the `cdist` call for the following data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "n1 = 50000\n",
    "n2 = 100\n",
    "p = 10\n",
    "XA = np.random.normal(0, 1, (n1, p))\n",
    "XB = np.random.normal(0, 1, (n2, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

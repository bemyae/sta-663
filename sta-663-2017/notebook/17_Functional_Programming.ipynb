{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import time\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with large data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lazy evaluation, pure functions and higher order functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lazy and eager evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list comprehension is **eager**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x*x for x in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator expression is **lazy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x11d3fca40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x*x for x in range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use generators as **iterators**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g = (x*x for x in range(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-5f315c5de15b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "next(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A generator is **single use**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in g:\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0, 1, 4, "
     ]
    }
   ],
   "source": [
    "g = (x*x for x in range(3))\n",
    "for i in g:\n",
    "    print(i, end=\", \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list constructor forces evaluation of the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 4]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(x*x for x in range(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An eager **function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eager_updown(n):\n",
    "    xs = []\n",
    "    for i in range(n):\n",
    "        xs.append(i)\n",
    "    for i in range(n, -1, -1):\n",
    "        xs.append(i)\n",
    "    return xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eager_updown(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lazy **generator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lazy_updown(n):\n",
    "    for i in range(n):\n",
    "        yield i\n",
    "    for i in range(n, -1, -1):\n",
    "        yield i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object lazy_updown at 0x11d6bd990>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lazy_updown(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 2, 1, 0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(lazy_updown(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure and impure functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pure function is like a mathematical function. Given the same inputs, it always returns the same output, and has no side effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pure(alist):\n",
    "    return [x*x for x in alist]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An impure function has **side effects**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def impure(alist):\n",
    "    for i in range(len(alist)):\n",
    "        alist[i] = alist[i]*alist[i]\n",
    "    return alist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = [1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3] [1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "ys = pure(xs)\n",
    "print(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9] [1, 4, 9]\n"
     ]
    }
   ],
   "source": [
    "ys = impure(xs)\n",
    "print(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quiz\n",
    "\n",
    "Say if the following functions are pure or impure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f1(n):\n",
    "    return n//2 if n % 2==0 else n*3+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f2(n):\n",
    "    return np.random.random(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f3(n):\n",
    "    n = 23\n",
    "    return n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f4(a, n=[]):\n",
    "    n.append(a)\n",
    "    return n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher order functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 4, 1, 10, 2, 16, 3, 22, 4, 28]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(f1, range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x % 2 == 0, range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, range(10), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5, 6]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(lambda x, y: x + y, [[1,2], [3,4], [5,6]], [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the operator module\n",
    "\n",
    "The `operator` module provides all the Python operators as functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator as op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce(op.mul, range(1, 6), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 5, 8]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(op.itemgetter(1), [[1,2,3],[4,5,6],[7,8,9]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3),\n",
       " (1, 2, 4),\n",
       " (1, 2, 5),\n",
       " (1, 3, 4),\n",
       " (1, 3, 5),\n",
       " (1, 4, 5),\n",
       " (2, 3, 4),\n",
       " (2, 3, 5),\n",
       " (2, 4, 5),\n",
       " (3, 4, 5)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(it.combinations(range(1,6), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate all Boolean combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 0),\n",
       " (0, 0, 1),\n",
       " (0, 1, 0),\n",
       " (0, 1, 1),\n",
       " (1, 0, 0),\n",
       " (1, 0, 1),\n",
       " (1, 1, 0),\n",
       " (1, 1, 1)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(it.product([0,1], repeat=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(it.starmap(op.add, zip(range(5), range(5))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(it.takewhile(lambda x: x < 3, range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 ['the', 'fox', 'the', 'dog']\n",
      "4 ['over', 'lazy']\n",
      "5 ['quick', 'brown', 'jumps']\n"
     ]
    }
   ],
   "source": [
    "data = sorted('the quick brown fox jumps over the lazy dog'.split(), key=len)\n",
    "for k, g in it.groupby(data, key=len):\n",
    "    print(k, list(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using toolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: toolz in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install toolz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import toolz as tz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 2), (3, 4, 5), (6, 7, 8)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tz.partition(3, range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 2), (3, 4, 5), (6, 7, 8), (9, None, None)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tz.partition(3, range(10), pad=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TGCGTACTTTTCGCTATCCTCTAGTAGTTG'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 30\n",
    "dna = ''.join(np.random.choice(list('ACTG'), n))\n",
    "dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'C'): 1,\n",
       " ('A', 'G'): 2,\n",
       " ('A', 'T'): 1,\n",
       " ('C', 'C'): 1,\n",
       " ('C', 'G'): 2,\n",
       " ('C', 'T'): 4,\n",
       " ('G', 'C'): 2,\n",
       " ('G', 'T'): 3,\n",
       " ('T', 'A'): 4,\n",
       " ('T', 'C'): 3,\n",
       " ('T', 'G'): 2,\n",
       " ('T', 'T'): 4}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz.frequencies(tz.sliding_window(2, dna))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using pipes and the curried namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from toolz import curried as c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'C'): 1,\n",
       " ('A', 'G'): 2,\n",
       " ('A', 'T'): 1,\n",
       " ('C', 'C'): 1,\n",
       " ('C', 'G'): 2,\n",
       " ('C', 'T'): 4,\n",
       " ('G', 'C'): 2,\n",
       " ('G', 'T'): 3,\n",
       " ('T', 'A'): 4,\n",
       " ('T', 'C'): 3,\n",
       " ('T', 'G'): 2,\n",
       " ('T', 'T'): 4}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz.pipe(\n",
    "    dna,    \n",
    "    c.sliding_window(2), # using curry\n",
    "    c.frequencies,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "composed = tz.compose(\n",
    "    c.frequencies,\n",
    "    c.sliding_window(2),  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'C'): 1,\n",
       " ('A', 'G'): 2,\n",
       " ('A', 'T'): 1,\n",
       " ('C', 'C'): 1,\n",
       " ('C', 'G'): 2,\n",
       " ('C', 'T'): 4,\n",
       " ('G', 'C'): 2,\n",
       " ('G', 'T'): 3,\n",
       " ('T', 'A'): 4,\n",
       " ('T', 'C'): 3,\n",
       " ('T', 'G'): 2,\n",
       " ('T', 'T'): 4}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "composed(dna)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing many sets of DNA strings without reading into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object <genexpr> at 0x11d6bddb0>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = 10000\n",
    "n = 300\n",
    "dnas = (''.join(np.random.choice(list('ACTG'), n, p=[.1, .2, .3, .4])) \n",
    "        for i in range(m))\n",
    "dnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('A', 'A'): 29994,\n",
       " ('A', 'C'): 59620,\n",
       " ('A', 'G'): 120072,\n",
       " ('A', 'T'): 89211,\n",
       " ('C', 'A'): 59860,\n",
       " ('C', 'C'): 119228,\n",
       " ('C', 'G'): 238914,\n",
       " ('C', 'T'): 179708,\n",
       " ('G', 'A'): 119757,\n",
       " ('G', 'C'): 239565,\n",
       " ('G', 'G'): 478575,\n",
       " ('G', 'T'): 359070,\n",
       " ('T', 'A'): 89249,\n",
       " ('T', 'C'): 179377,\n",
       " ('T', 'G'): 359409,\n",
       " ('T', 'T'): 268391}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tz.merge_with(sum, \n",
    "              tz.map(\n",
    "                  composed,\n",
    "                  dnas\n",
    "              )\n",
    "             )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with out-of-core memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `memmap`\n",
    "\n",
    "You can selectively retrieve parts of `numpy` arrays  stored on disk into memory for processing with `memmap`. \n",
    "\n",
    "Memory-mapped files are used for accessing small segments of large files on disk, without reading the entire file into memory. The `numpy.memmap` can be used anywhere an `ndarray` is used. The maximum size of a `memmap` array is limited by what the operating system allows - in particular, this is different on 32 and 64 bit architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the memory mapped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "filename = 'random.dat'\n",
    "shape = (n, 1000, 1000)\n",
    "\n",
    "# create memmap\n",
    "fp = np.memmap(filename, dtype='float64', mode='w+', shape=shape)\n",
    "\n",
    "# store some data in it\n",
    "for i in range(n):\n",
    "    x = np.random.random(shape[1:])\n",
    "    fp[i] = x\n",
    "    \n",
    "# flush to disk and remove file handler\n",
    "del fp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usign the memory mapped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.500001143637 Total: 0.64s Per file: 0.01s\n"
     ]
    }
   ],
   "source": [
    "fp = np.memmap(filename, dtype='float64', mode='r', shape=shape) \n",
    "\n",
    "# only one block is retreived into memory at a time\n",
    "start = time.time()\n",
    "xs = [fp[i].mean() for i in range(n)]\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(np.mean(xs), 'Total: %.2fs Per file: %.2fs' % (elapsed, elapsed/n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using HDF5\n",
    "\n",
    "HDF5 is a hierarchical file format that allows selective disk reads, but also provides a tree structure for organizing your data sets. It can also include metadata annotation for documentation. Because of its flexibility, you should seriously consider using HDF5 for your data storage needs.\n",
    "\n",
    "I suggest using the python package `h5py` for working with HDF5 files. See [documentation](http://docs.h5py.org/en/latest/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating an HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 701 ms, sys: 124 ms, total: 825 ms\n",
      "Wall time: 920 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "n = 5\n",
    "filename = 'random.hdf5'\n",
    "shape = (n, 1000, 1000)\n",
    "\n",
    "groups = ['Sim%02d' % i for i in range(5)]\n",
    "\n",
    "with h5py.File(filename, 'w') as f:\n",
    "    # Create hierarchical group structure\n",
    "    for group in groups:\n",
    "        g = f.create_group(group)\n",
    "        # Add metadata for each group\n",
    "        g.attrs['created'] = str(datetime.datetime.now())\n",
    "        \n",
    "        # Save 100 arrays in each group\n",
    "        for i in range(n):\n",
    "            x = np.random.random(shape[1:]) \n",
    "            dset = g.create_dataset('x%06d' % i, shape=x.shape)\n",
    "            dset[:] = x\n",
    "            # Add metadata for each array\n",
    "            dset.attrs['created'] = str(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using an HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = h5py.File('random.hdf5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HDF5 objects can be treated like dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim00\n",
      "Sim01\n",
      "Sim02\n",
      "Sim03\n",
      "Sim04\n"
     ]
    }
   ],
   "source": [
    "for name in f:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim00\n",
      "Sim01\n",
      "Sim02\n",
      "Sim03\n",
      "Sim04\n"
     ]
    }
   ],
   "source": [
    "for key in f.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sim1 = f.get('Sim01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x000000', 'x000001', 'x000002', 'x000003', 'x000004']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sim1.keys())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or recursed through like trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sim00\n",
      "Sim00/x000000\n",
      "Sim00/x000001\n",
      "Sim00/x000002\n",
      "Sim00/x000003\n",
      "Sim00/x000004\n",
      "Sim01\n",
      "Sim01/x000000\n",
      "Sim01/x000001\n",
      "Sim01/x000002\n",
      "Sim01/x000003\n",
      "Sim01/x000004\n",
      "Sim02\n",
      "Sim02/x000000\n",
      "Sim02/x000001\n",
      "Sim02/x000002\n",
      "Sim02/x000003\n",
      "Sim02/x000004\n",
      "Sim03\n",
      "Sim03/x000000\n",
      "Sim03/x000001\n",
      "Sim03/x000002\n",
      "Sim03/x000003\n",
      "Sim03/x000004\n",
      "Sim04\n",
      "Sim04/x000000\n",
      "Sim04/x000001\n",
      "Sim04/x000002\n",
      "Sim04/x000003\n",
      "Sim04/x000004\n"
     ]
    }
   ],
   "source": [
    "f.visit(lambda x: print(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving data and attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-03-29 16:19:19.613896'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim2 = f.get('Sim02')\n",
    "sim2.attrs['created']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1000)\n",
      "float32\n",
      "['created']\n",
      "2017-03-29 16:19:19.749793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49980888"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = sim2.get('x000003')\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "print(list(x.attrs.keys()))\n",
    "print(x.attrs['created'])\n",
    "np.mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using SQLite3\n",
    "\n",
    "When data is on a relational database, it is useful to do as much preprocessing as possible using SQL - this will be performed using highly efficient compiled routines on the (potentially remote) computer where the database exists.\n",
    "\n",
    "Here we will use SQLite3 together with `pandas` to summarize a (potentially) large database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "engine = create_engine('sqlite:///data/movies.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>number</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1996</td>\n",
       "      <td>1375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000</td>\n",
       "      <td>1365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1998</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2002</td>\n",
       "      <td>1360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1997</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   year  number\n",
       "0  1996    1375\n",
       "1  2000    1365\n",
       "2  1998    1360\n",
       "3  2002    1360\n",
       "4  1997    1335"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = '''\n",
    "SELECT year, count(*) as number \n",
    "FROM data \n",
    "GROUP BY year \n",
    "ORDER BY number DESC\n",
    "'''\n",
    "\n",
    "# The coounting, grouping and sorting is done by the database, not pandas\n",
    "# So this query will work even if the movies dataabse is many terabytes in size\n",
    "df = pd.read_sql_query(q, engine)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-memory data conversions\n",
    "\n",
    "There is a convenient Python package called `odo` that will convert data between different formats without having to load all the data into memory first.  This allows conversion of potentially huge files.\n",
    "\n",
    "[Odo](http://www.startrek.com/database_article/odo) is a shape shifting character in the Star Trek universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: odo in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages\n",
      "Requirement already satisfied: datashape>=0.5.0 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from odo)\n",
      "Requirement already satisfied: numpy>=1.7 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from odo)\n",
      "Requirement already satisfied: pandas>=0.15.0 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from odo)\n",
      "Requirement already satisfied: toolz>=0.7.3 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from odo)\n",
      "Requirement already satisfied: multipledispatch>=0.4.7 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from odo)\n",
      "Requirement already satisfied: networkx in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from odo)\n",
      "Requirement already satisfied: python-dateutil in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from datashape>=0.5.0->odo)\n",
      "Requirement already satisfied: pytz>=2011k in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from pandas>=0.15.0->odo)\n",
      "Requirement already satisfied: decorator>=3.4.0 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from networkx->odo)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from python-dateutil->datashape>=0.5.0->odo)\n"
     ]
    }
   ],
   "source": [
    "! pip install odo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import odo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<odo.backends.csv.CSV at 0x1503c7828>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odo.odo('sqlite:///data/movies.db::data', 'data/movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index,movieId,title,genres,year\r\n",
      "0,1,\"Toy Story (1995)\",Adventure|Animation|Children|Comedy|Fantasy,1995\r\n",
      "1,2,\"Jumanji (1995)\",Adventure|Children|Fantasy,1995\r\n",
      "2,3,\"Grumpier Old Men (1995)\",Comedy|Romance,1995\r\n",
      "3,4,\"Waiting to Exhale (1995)\",Comedy|Drama|Romance,1995\r\n",
      "4,5,\"Father of the Bride Part II (1995)\",Comedy,1995\r\n",
      "5,6,\"Heat (1995)\",Action|Crime|Thriller,1995\r\n",
      "6,7,\"Sabrina (1995)\",Comedy|Romance,1995\r\n",
      "7,8,\"Tom and Huck (1995)\",Adventure|Children,1995\r\n",
      "8,9,\"Sudden Death (1995)\",Action,1995\r\n"
     ]
    }
   ],
   "source": [
    "! head data/movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilistic data structures\n",
    "\n",
    "A `data sketch` is a probabilistic algorithm or data structure that approximates some statistic of interest, typically using very little memory and processing time. Often they are applied to streaming data, and so must be able to incrementally process data. Many data sketches make use of hash functions to distribute data into buckets uniformly. Typically, data sketches have the following desirable properties\n",
    "\n",
    "- sub-linear in space\n",
    "- single scan\n",
    "- can be parallelized\n",
    "- can be combined (merge)\n",
    "\n",
    "Examples where counting distinct values is useful:\n",
    "\n",
    "- number of unique users in a Twitter stream\n",
    "- number of distinct records to be fetched by a database query\n",
    "- number of unique IP addresses accessing a website\n",
    "- number of distinct queries submitted to a search engine\n",
    "- number of distinct DNA motifs in genomics data sets (e.g. microbiome)\n",
    "\n",
    "Packages for data sketches in Python are relatively immature, and if you are interested, you could make a large contribution by creating a comprehensive open source library of data sketches in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HyperLogLog\n",
    "\n",
    "Counting the number of **distinct** elements exactly requires storage of all distinct elements (e.g. in a set) and hence grows with the cardinality $n$. Probabilistic data structures known as Distinct Value Sketches can do this with a tiny and fixed memory size.\n",
    "\n",
    "A hash function takes data of arbitrary size and converts it into a number in a fixed range. Ideally, given an arbitrary set of data items, the hash function generates numbers that follow a uniform distribution within the fixed range. Hash functions are immensely useful throughout computer science (for example - they power Python sets and dictionaries), and especially for the generation of probabilistic data structures.\n",
    "\n",
    "The binary digits in a (say) 32-bit hash are effectively random, and equivalent to a sequence of fair coin tosses. Hence the probability that we see a run of 5 zeros in the smallest hash so far suggests that we have added $2^5$ unique items so far. This is the intuition behind the loglog family of Distinct Value Sketches. Note that the biggest count we can track with 32 bits is $2^{32} = 4294967296$.\n",
    "\n",
    "The accuracy of the sketch can be improved by averaging results with multiple coin flippers. In practice, this is done by using the first $k$ bit registers to identify $2^k$ different coin flippers. Hence, the max count is now $2 ** (32 - k)$. The hyperloglog algorithm uses the harmonic mean of the $2^k$ flippers which reduces the effect of outliers and hence the variance of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hyperloglog in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install hyperloglog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hyperloglog import HyperLogLog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare unique counts with set and hyperloglog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\t\tHLL\t\tRel Error\n",
      "       1\t       1\t\t0.000\n",
      "    7095\t    7151\t\t0.003\n",
      "   11582\t   11667\t\t0.002\n",
      "   16010\t   16202\t\t0.003\n",
      "   20058\t   20296\t\t0.003\n",
      "   23761\t   24087\t\t0.003\n",
      "   27463\t   27785\t\t0.003\n",
      "   29780\t   30173\t\t0.003\n",
      "   33740\t   34206\t\t0.003\n",
      "   38070\t   38473\t\t0.002\n",
      "   41599\t   42235\t\t0.003\n",
      "   44119\t   44619\t\t0.002\n",
      "   48549\t   49197\t\t0.003\n",
      "   49511\t   50191\t\t0.003\n"
     ]
    }
   ],
   "source": [
    "def flatten(xs):\n",
    "    return (x for sublist in xs for x in sublist)\n",
    "\n",
    "def error(a, b, n):\n",
    "    return abs(len(a) - len(b))/n\n",
    "\n",
    "print('True\\t\\tHLL\\t\\tRel Error')\n",
    "with open('data/Ulysses.txt') as f:\n",
    "    word_list = flatten(line.split() for line in f)\n",
    "    \n",
    "    s = set([])\n",
    "    hll = HyperLogLog(error_rate=0.01)\n",
    "    for i, word in enumerate(word_list):\n",
    "        s.add(word)\n",
    "        hll.add(word)\n",
    "        if i%int(.2e5)==0:\n",
    "            print('%8d\\t%8d\\t\\t%.3f' % \n",
    "                  (len(s), len(hll),\n",
    "                   0 if i==0 else error(s, hll, i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bloom filters\n",
    "\n",
    "Bloom filters are designed to answer queries about whether a specific item is in a collection. If the answer is NO, then it is definitive. However, if the answer is yes, it might be a false positive. The possibility of a false positive makes the Bloom filter a probabilistic data structure.\n",
    "\n",
    "A bloom filter consists of a bit vector of length $k$ initially set to zero, and $n$ different hash functions that return a hash value that will fall into one of the $k$ bins. In the construction phase, for every item in the collection, $n$ hash values are generated by the $n$ hash functions, and every position indicated by a hash value is flipped to one. In the query phase, given an item, $n$ hash values are calculated as before - if any of these $n$ positions is a zero, then the item is definitely not in the collection. However, because of the possibility of hash collisions, even if all the positions are one, this could be a false positive. Clearly, the rate of false positives depends on the ratio of zero and one bits, and there are Bloom filter implementations that will dynamically bound the ratio and hence the false positive rate.\n",
    "\n",
    "Possible uses of a Bloom filter include:\n",
    "\n",
    "- Does a particular sequence motif appear in a DNA string?\n",
    "- Has this book been recommended to this customer before?\n",
    "- Check if an element exists on disk before performing I/O\n",
    "- Check if URL is a potential malware site using in-browser Bloom filter to minimize network communication\n",
    "- As an alternative way to generate distinct value counts cheaply (only increment count if Bloom filter says NO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/jaybaird/python-bloomfilter.git\n",
      "  Cloning https://github.com/jaybaird/python-bloomfilter.git to /private/var/folders/xf/rzdg30ps11g93j3w0h589q780000gn/T/pip-4g7j8fys-build\n",
      "  Requirement already satisfied (use --upgrade to upgrade): pybloom==2.0.0 from git+https://github.com/jaybaird/python-bloomfilter.git in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages\n",
      "Requirement already satisfied: bitarray>=0.3.4 in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages (from pybloom==2.0.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install git+https://github.com/jaybaird/python-bloomfilter.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pybloom import ScalableBloomFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Scalable Bloom Filter grows as needed to keep the error rate small \n",
    "sbf = ScalableBloomFilter(error_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/Ulysses.txt') as f:\n",
    "    word_set = set(flatten(line.split() for line in f))\n",
    "    for word in word_set:\n",
    "        sbf.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ask Bloom filter if test words were in Ulysses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_words = ['banana', 'artist', 'Dublin', 'masochist', 'Obama']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana True\n",
      "artist True\n",
      "Dublin True\n",
      "masochist False\n",
      "Obama False\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    print(word, word in sbf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "banana True\n",
      "artist True\n",
      "Dublin True\n",
      "masochist False\n",
      "Obama False\n"
     ]
    }
   ],
   "source": [
    "for word in test_words:\n",
    "    print(word, word in word_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small-scale distributed programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `dask`\n",
    "\n",
    "For data sets that are not too big (say up to 1 TB), it is typically sufficient to process on a single workstation. The package dask provides 3 data structures that mimic regular Python data structures but perform computation in a distributed way allowing you to make optimal use of  multiple cores easily.\n",
    "\n",
    "These structures are\n",
    "\n",
    "- dask array ~ numpy array\n",
    "- dask bag ~ Python dictionary\n",
    "- dask dataframe ~ pandas dataframe\n",
    "\n",
    "From the [official documentation](http://dask.pydata.org/en/latest/index.html), \n",
    "\n",
    "```\n",
    "Dask is a simple task scheduling system that uses directed acyclic graphs (DAGs) of tasks to break up large computations into many small ones.\n",
    "\n",
    "Dask enables parallel computing through task scheduling and blocked algorithms. This allows developers to write    complex parallel algorithms and execute them in parallel either on a modern multi-core machine or on a distributed cluster.\n",
    "\n",
    "On a single machine dask increases the scale of comfortable data from fits-in-memory to fits-on-disk by intelligently streaming data from disk and by leveraging all the cores of a modern CPU.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dask in /Users/cliburn/anaconda2/envs/p3/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dask` arrays\n",
    "\n",
    "These behave like `numpy` arrays, but break a massive job into **tasks** that are then executed by a **scheduler**. The default scheduler uses threading but you can also use multiprocessing or distributed or even serial processing (mainly for debugging). You can tell the dask array how to break the data into **chunks** for processing.\n",
    "\n",
    "From official documents\n",
    "\n",
    "```\n",
    "For performance, a good choice of chunks follows the following rules:\n",
    "\n",
    "A chunk should be small enough to fit comfortably in memory. We’ll have many chunks in memory at once.\n",
    "A chunk must be large enough so that computations on that chunk take significantly longer than the 1ms overhead per task that dask scheduling incurs. A task should take longer than 100ms.\n",
    "Chunks should align with the computation that you want to do. For example if you plan to frequently slice along a particular dimension then it’s more efficient if your chunks are aligned so that you have to touch fewer chunks. If you want to add two arrays then its convenient if those arrays have matching chunks patterns.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We resuse the 100 * 1000 * 1000 random numbers in the memmap file on disk\n",
    "n = 100\n",
    "filename = 'random.dat'\n",
    "shape = (n, 1000, 1000)\n",
    "fp = np.memmap(filename, dtype='float64', mode='r', shape=shape)\n",
    "\n",
    "# We can decide on the chunk size to be distributed for computing\n",
    "xs = [da.from_array(fp[i], chunks=(200,500)) for i in range(n)]\n",
    "xs = da.concatenate(xs)\n",
    "avg = xs.mean().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50000114363730053"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Typically we store Dask arrays inot HDF5\n",
    "\n",
    "da.to_hdf5('data/xs.hdf5', '/foo/xs', xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 1000)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File('data/xs.hdf5', 'r') as f:\n",
    "    print(f.get('/foo/xs').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dask` data frames\n",
    "\n",
    "Dask dataframes can treat multiple pandas dataframes that might not simultaneously fit into memory like a single dataframe. See use of globbing to specify multiple source files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    f = 'data/x%03d.csv' % i\n",
    "    np.savetxt(f, np.random.random((1000, 5)), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 0            1            2            3            4\n",
      "count  5000.000000  5000.000000  5000.000000  5000.000000  5000.000000\n",
      "mean      0.492787     0.498913     0.506285     0.497849     0.496456\n",
      "std       0.290051     0.290067     0.289115     0.287252     0.288440\n",
      "min       0.000120     0.000127     0.001602     0.000124     0.000028\n",
      "25%       0.270114     0.261522     0.267837     0.262800     0.262552\n",
      "50%       0.511363     0.525779     0.522839     0.515185     0.511809\n",
      "75%       0.762856     0.755237     0.761059     0.761570     0.772811\n",
      "max       0.999914     0.999940     0.999997     0.999718     0.999783\n"
     ]
    }
   ],
   "source": [
    "df = dd.read_csv('data/x*.csv', header=None)\n",
    "print(df.describe().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `dask` bags\n",
    "\n",
    "\n",
    "Dask bags work like dictionaries for unstructured or semi-structured data sets, typically over many files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### The AA subdirectory consists of 101 1 MB plain text files from the English Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = db.read_text('data/wiki/AA/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 12s, sys: 2.27 s, total: 1min 14s\n",
      "Wall time: 1min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = text.str.split().concat().frequencies().topk(10, key=lambda x: x[1])\n",
    "top10 = words.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1051994), ('of', 617239), ('and', 482039), ('in', 370266), ('to', 356495), ('a', 312597), ('is', 174145), ('as', 145215), ('was', 141788), ('The', 141724)]\n"
     ]
    }
   ],
   "source": [
    "print(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is slow because of disk access. Fix by changing scheduler to work asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11.1 s, sys: 413 ms, total: 11.6 s\n",
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "words = text.str.split().concat().frequencies().topk(10, key=lambda x: x[1])\n",
    "top10 = words.compute(get = dask.async.get_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 1051994), ('of', 617239), ('and', 482039), ('in', 370266), ('to', 356495), ('a', 312597), ('is', 174145), ('as', 145215), ('was', 141788), ('The', 141724)]\n"
     ]
    }
   ],
   "source": [
    "print(top10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Conversion from bag to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "freqs = (text.\n",
    "         str.translate({ord(char): None for char in string.punctuation}).\n",
    "         str.lower().\n",
    "         str.split().\n",
    "         concat().\n",
    "         frequencies())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get the top 5 words sorted by key (not value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🚲', 1), ('𝛿', 2), ('𓏘𓃭𓇋𓍯𓊪𓄿𓂧𓂋𓄿𓏏𓆇', 1), ('𒀸𒋗𒁺', 1), ('𐑅', 1)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freqs.topk(5).compute(get = dask.async.get_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>€53</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doodnath</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>iphone</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flagged</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>desks</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word   n\n",
       "0       €53   1\n",
       "1  doodnath   1\n",
       "2    iphone  82\n",
       "3   flagged   3\n",
       "4     desks  10"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_freqs = freqs.to_dataframe(columns=['word', 'n'])\n",
    "df_freqs.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The compute method converts to a regular pandas dataframe\n",
    "\n",
    "For data sets that fit in memory, pandas is faster and allows some operations like sorting that are not provided by dask dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df_freqs.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16770</th>\n",
       "      <td>🚲</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313069</th>\n",
       "      <td>𝛿</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137307</th>\n",
       "      <td>𓏘𓃭𓇋𓍯𓊪𓄿𓂧𓂋𓄿𓏏𓆇</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103308</th>\n",
       "      <td>𒀸𒋗𒁺</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326342</th>\n",
       "      <td>𐑅</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               word  n\n",
       "16770             🚲  1\n",
       "313069            𝛿  2\n",
       "137307  𓏘𓃭𓇋𓍯𓊪𓄿𓂧𓂋𓄿𓏏𓆇  1\n",
       "103308          𒀸𒋗𒁺  1\n",
       "326342            𐑅  1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('word', ascending=False).head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

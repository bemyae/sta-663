{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# TensorFlow and Edward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Packages\n",
    "\n",
    "- [TensorFlow](https://www.tensorflow.org)\n",
    "- [TFLearn](https://github.com/tflearn/tflearn)\n",
    "- [Keras](https://github.com/fchollet/keras)\n",
    "- [Edward](http://edwardlib.org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## References\n",
    "\n",
    "- [A Tour of TensorFlow](https://arxiv.org/pdf/1610.01178.pdf)\n",
    "- [Data Flow Graphs Intro](http://web.cecs.pdx.edu/~mperkows/temp/JULY/data-flow-graph.pdf)\n",
    "- [Edward: A library for probabilistic modeling, inference, and criticism](https://arxiv.org/pdf/1610.09787.pdf)\n",
    "- [Deep Probabilistic Programming](https://arxiv.org/pdf/1701.03757.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensorFlow\n",
    "\n",
    "- A Python/C++/Go framework for compiling and executing mathematical expressions\n",
    "- First released by Google in 2015\n",
    "- Based on Data Flow Graphs\n",
    "- Widely used to implement Deep Neural Networks (DNN)\n",
    "- *Edward* uses TensorFlow to implement a Probabilistic Programming Language (PPL)\n",
    "- Can distribute computation to *multiple computers*, each of which potentially has *multiple CPU, GPU or TPU devices*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data flow graph\n",
    "\n",
    "- Operations\n",
    "- Tensors\n",
    "- Constants, variables, placeholders\n",
    "- Sessions\n",
    "\n",
    "![DFG](figs/data_flow.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Execution model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Agents\n",
    "\n",
    "- Client\n",
    "- Master\n",
    "- Workers\n",
    "- Devices\n",
    "\n",
    "![Agents](figs/agents.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Placement algorithm\n",
    "\n",
    "- Kernel on device?\n",
    "- Size of input and output tensors\n",
    "- Expected execution time\n",
    "- Heuristic for cross-device transmission time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optimization 1:  Common subgraph elimination\n",
    "\n",
    "Before             |  After\n",
    ":-------------------------:|:-------------------------:\n",
    "![Elimination](figs/elim1.png)  |  ![Elimination](figs/elim2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Optimization 2:  As late as possible (ALAP) scheduling\n",
    "\n",
    "ASAP             |  ALAP\n",
    ":-------------------------:|:-------------------------:\n",
    "![ASAP](figs/asap.png)  |  ![ALAP](figs/alap.png)\n",
    "\n",
    "- Lossy compression for cross-device transmission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Automatic differentiation\n",
    "\n",
    "- Symbol-to-symbol calculation of gradient\n",
    "- Used for back-propagation in neural networks\n",
    "- Used for gradient based optimization, HMC etc in Edward\n",
    "\n",
    "![Chain rule](figs/auto_diff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other features\n",
    "\n",
    "- Control flow (if and while) - enable recursion and cycles\n",
    "- Checkpoints\n",
    "  - `save`\n",
    "  - `restore`\n",
    "- TensorBoard visualization\n",
    "  - Graphs\n",
    "  - Scalar summaries (e.g. evaluation metrics)\n",
    "  - Histogram summaries (e.g. weight distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Abstraction layers\n",
    "\n",
    "- Deep Neural Networks\n",
    "  - `contrib.learn`\n",
    "  - `tflearn`\n",
    "  - `tf-slim`\n",
    "  - `keras`\n",
    "- Probabilistic Programming Language\n",
    "  - `edward`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## TensorFlow Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hello world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "h = tf.constant('Hello')\n",
    "w = tf.constant(' world!')\n",
    "hw = h + w\n",
    "\n",
    "with tf.Session() as s:\n",
    "    ans = s.run(hw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Hello world!'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Arithmetic on data flow graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 3, 10, 5, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = tf.multiply(a, b)\n",
    "e = tf.add(b, c)\n",
    "f = tf.subtract(d, e)\n",
    "\n",
    "with tf.Session() as s:\n",
    "    fetches = [a, b, c, d, e, f]\n",
    "    ans = s.run(fetches)\n",
    "    \n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 2, 3, 10, 5, 5]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.constant(5)\n",
    "b = tf.constant(2)\n",
    "c = tf.constant(3)\n",
    "d = a * b\n",
    "e = b + c\n",
    "f = d - e\n",
    "\n",
    "with tf.Session() as s:\n",
    "    fetrches = [a,b,c,d,e,f]\n",
    "    ans = s.run(fetches)\n",
    "    \n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.055994"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data = np.random.randn(5, 10)\n",
    "w_data = np.random.randn(10, 1)\n",
    "\n",
    "x = tf.placeholder('float32', (5, 10))\n",
    "w = tf.placeholder('float32', (10, 1))\n",
    "b = tf.fill((5,1), -1.0)\n",
    "\n",
    "xwb = tf.matmul(x, w) + b\n",
    "v = tf.reduce_max(xwb)\n",
    "\n",
    "with tf.Session() as s:\n",
    "    ans = s.run(v, feed_dict={x: x_data, w: w_data})\n",
    "    \n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear regreession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "n, p = 1000, 3\n",
    "\n",
    "α = -1.0\n",
    "β = np.reshape([0.5, 0.2, 1.0], (3,1))\n",
    "x_data = np.random.randn(n, p)\n",
    "y = α + x_data @ β + np.random.randn(n, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder('float32', [None, p])\n",
    "y_true = tf.placeholder('float32', [None, 1])\n",
    "\n",
    "a = tf.Variable(0.0, dtype='float32')\n",
    "b = tf.Variable(np.zeros((3,1), dtype='float32'))\n",
    "\n",
    "y_pred = a + tf.matmul(x, b)\n",
    "\n",
    "ϵ = 0.5\n",
    "loss = tf.reduce_mean(tf.square(y_true - y_pred))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=ϵ)\n",
    "train = optimizer.minimize(loss)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=1\n",
      "a = -0.9617397785186768\n",
      "b = [ 0.58339775  0.21545935  0.97747314]\n",
      "\n",
      "iter=2\n",
      "a = -0.9741984009742737\n",
      "b = [ 0.57611543  0.22001249  1.00742912]\n",
      "\n",
      "iter=3\n",
      "a = -0.9753177762031555\n",
      "b = [ 0.57605499  0.2206755   1.00737011]\n",
      "\n",
      "iter=4\n",
      "a = -0.9753146171569824\n",
      "b = [ 0.57600915  0.22066851  1.00741804]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "steps = 5\n",
    "with tf.Session() as session:\n",
    "    session.run(init)\n",
    "    \n",
    "    for i in range(1, steps):\n",
    "        \n",
    "        session.run(train, feed_dict={y_true: y, x: x_data})\n",
    "        \n",
    "        if i% 1 == 0:\n",
    "            a_, b_ = session.run([a, b])\n",
    "            print('iter={}'.format(i))\n",
    "            print('a = {}'.format(a_))\n",
    "            print('b = {}'.format(b_.ravel()))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MNIST digits classificaiton (canonical toy example)\n",
    "    \n",
    "Collection of $28 \\times 28$ pixel images of hand-written digits. Objective is to classify image into one of ten possile classes. [State of the art DNN methods](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html) can achieve accuracy of approxmately 99.8% accuracy.\n",
    "   \n",
    "![Digits](https://camo.githubusercontent.com/d440ac2eee1cb3ea33340a2c5f6f15a0878e9275/687474703a2f2f692e7974696d672e636f6d2f76692f3051493378675875422d512f687164656661756c742e6a7067)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Download the data using `input_data` from `tutorials.mnist`\n",
    "2. Declare x, W, y_true and y_pred\n",
    "3. Define loss function\n",
    "4. Define minimization algorithm\n",
    "5. Define evaluation metrics\n",
    "6. Start a session to\n",
    "    1. Run loop for minimization of batches\n",
    "    2. Run evaluation metric on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "n, p = 784, 10\n",
    "steps = 1000\n",
    "batch_size = 100\n",
    "alpha = 0.5\n",
    "\n",
    "data_dir = '/tmp/data'\n",
    "\n",
    "data = input_data.read_data_sets(data_dir, one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, n])\n",
    "W = tf.Variable(tf.zeros([n, p]))\n",
    "\n",
    "y_true = tf.placeholder(tf.float32, [None, 10])\n",
    "y_pred = tf.matmul(x, W)\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y_true))\n",
    "\n",
    "gd = tf.train.GradientDescentOptimizer(alpha).minimize(loss)\n",
    "\n",
    "correct_mask = tf.equal(tf.arg_max(y_pred, 1), tf.arg_max(y_true, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session() as s:\n",
    "    s.run(tf.global_variables_initializer())\n",
    "\n",
    "    # train\n",
    "    for i in range(steps):\n",
    "        batch_xs, batch_ys = data.train.next_batch(batch_size)\n",
    "        s.run(gd, feed_dict={x: batch_xs, y_true: batch_ys})\n",
    "        \n",
    "    # test\n",
    "    ans = s.run(accuracy, feed_dict={x: data.test.images, y_true: data.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91790003"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Using the `tflearn` abstraction layer\n",
    "\n",
    "This just implements logistic regresion. Note that we can get much better perofrmance using DNN, but that is not covered here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "! pip install --quiet tflearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 2579  | total loss: \u001b[1m\u001b[32m0.25304\u001b[0m\u001b[0m | time: 28.264s\n",
      "| SGD | epoch: 003 | loss: 0.25304 -- iter: 54976/55000\n",
      "Training Step: 2580  | total loss: \u001b[1m\u001b[32m0.24640\u001b[0m\u001b[0m | time: 29.406s\n",
      "| SGD | epoch: 003 | loss: 0.24640 | val_loss: 0.28411 -- iter: 55000/55000\n",
      "--\n"
     ]
    }
   ],
   "source": [
    "import tflearn\n",
    "import tflearn.datasets.mnist as mnist\n",
    "\n",
    "X, Y, validX, validY = mnist.load_data(one_hot=True)\n",
    "\n",
    "# Building our neural network\n",
    "input_layer = tflearn.input_data(shape=[None, 784])\n",
    "output_layer = tflearn.fully_connected(input_layer, 10, activation='softmax')\n",
    "\n",
    "# Optimization\n",
    "sgd = tflearn.SGD(learning_rate=0.5)\n",
    "net = tflearn.regression(output_layer, optimizer=sgd)\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(net, tensorboard_verbose=3)\n",
    "model.fit(X, Y, validation_set=(validX, validY), n_epoch=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.92030000000000001]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(validX, validY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Edward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overview\n",
    "\n",
    "- Named after George *Edward* Pelham Box\n",
    "\n",
    "![Model](figs/box_loop.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "- `numpy` arrays or `tensorflow` tensors\n",
    "- `tnesorflow` placeholders\n",
    "- `tensorflow` data readers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model\n",
    "\n",
    "- A model is a joint distribution $p(x, z)$ of data $x$ and latent variables $z$\n",
    "- A random variable has a distribution parametrized by a parameter tensor $\\theta^*$\n",
    "- Each random variable is associated to a tenor\n",
    "$$\n",
    "x^* \\sim p(x \\mid \\theta^*)\n",
    "$$\n",
    "- Random variables can be combined with other TensorFlow operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Models are built by composing random variables\n",
    "\n",
    "Beta-Bernoulli Model      |\n",
    ":-------------------------:|\n",
    "![Model](figs/bb1.png)  |\n",
    "![Graph](figs/bb2.png) |\n",
    "![Code](figs/bb3.png) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Types of models\n",
    "\n",
    "- Directed graphical models\n",
    "- Neural networks\n",
    "- Bayesian non-parametric models\n",
    "- Probabilistic programs (stochastic control flow with contingent dependencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Inference\n",
    "\n",
    "- Posterior inference\n",
    "$$\n",
    "q(z, \\beta; \\lambda) \\approx p(z, \\beta | x)\n",
    "$$\n",
    "- Parameter estimation\n",
    "$$\n",
    "\\text{optimize} \\; \\hat{\\theta} \\leftarrow p(x; \\theta)\n",
    "$$\n",
    "- Conditional inference\n",
    "$$\n",
    "q(\\beta)q(z) \\approx p(z, \\beta \\mid x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Methods for inference\n",
    "\n",
    "- Variational inference\n",
    "  - MAP is a special case with point mass RVs\n",
    "- Monte Carlo\n",
    "- Composition of inference\n",
    "  - Hybrid algorithms (e.g. EM variants)\n",
    "  - Message passing algorithms  (e.g. expectation propagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![Methods](figs/inference.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Criticize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Point-based evaluations\n",
    "\n",
    "- Evaluation metrics\n",
    "  - Classification error\n",
    "  - Mean absolute error\n",
    "  - Log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Posterior predictive checks (PPC)\n",
    "\n",
    "- Posterior predictive distribution\n",
    "$$\n",
    "p(x_\\text{new} \\mid x) = \\int{p(x_\\text{new} \\mid z) p(z \\mid x) dz}\n",
    "$$\n",
    "- Procedure\n",
    "  - Draw sample from posterior predictive distribution\n",
    "  - Calculate test statistic on sample (e.g. mean, max)\n",
    "  - Repeat to get distribution of statistic\n",
    "  - Compare test statistic on *original* data to distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Edward examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Linear Regreessiion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import edward as ed\n",
    "from edward.models import Normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n, p = 1000, 3\n",
    "\n",
    "α = -1.0\n",
    "β = np.reshape([0.5, 0.2, 1.0], (3,1))\n",
    "\n",
    "# data for training\n",
    "x_train = np.random.randn(n, p)\n",
    "y_train = α + x_train @ β + np.random.normal(0, 1, (n,1))\n",
    "y_train = y_train.ravel()\n",
    "\n",
    "# data for testing\n",
    "x_test =  np.random.randn(n, p)\n",
    "y_test = α + x_test @ β + np.random.normal(0, 1, (n,1))\n",
    "y_test= y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model\n",
    "\n",
    "Given data $(x, y)$,\n",
    "\n",
    "$$\n",
    "p(w) = \\mathcal{N}(w, 0, 1) \\\\\n",
    "p(b) = \\mathcal{N}(b, 0, 1) \\\\\n",
    "p(y \\mid w, b, x) = \\prod_{i=1}^{n} \\mathcal{N}(y_i \\mid x_i^T w + b, 1)\n",
    "$$\n",
    "\n",
    "Note that we label the intercept $\\alpha$ as the bias $b$ and the coefficeints $\\beta$ as weights $w$ following neural network conventions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [n, p])\n",
    "w = Normal(mu=tf.zeros(p), sigma=tf.ones(p))\n",
    "b = Normal(mu=tf.zeros(1), sigma=tf.ones(1))\n",
    "y = Normal(mu=ed.dot(X, w) + b, sigma=tf.ones(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Inference\n",
    "\n",
    "We fit a fully factroized variational model by minimizing the Kullback-Leibler divergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "qw = Normal(mu=tf.Variable(tf.random_normal([p])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([p]))))\n",
    "qb = Normal(mu=tf.Variable(tf.random_normal([1])),\n",
    "            sigma=tf.nn.softplus(tf.Variable(tf.random_normal([1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [100%] ██████████████████████████████ Elapsed: 7s | Loss: 1438.422\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
     ]
    }
   ],
   "source": [
    "inference = ed.KLqp({w: qw, b: qb}, data={X: x_train, y: y_train})\n",
    "inference.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Criticism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Find the posterior predictive distrbution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_post = ed.copy(y, {w: qw, b: qb})\n",
    "# This is equivalent to\n",
    "# y_post = Normal(mu=ed.dot(X, qw) + qb, sigma=tf.ones(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Calculate evalution metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error on test data:\n",
      "1.03445\n",
      "Mean absolute error on test data:\n",
      "0.813144\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean squared error on test data:\")\n",
    "print(ed.evaluate('mean_squared_error', data={X: x_test, y_post: y_test}))\n",
    "\n",
    "print(\"Mean absolute error on test data:\")\n",
    "print(ed.evaluate('mean_absolute_error', data={X: x_test, y_post: y_test}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check parameters (true, prior, posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 0.5]), -0.39793062, 0.52038413),\n",
       " (array([ 0.2]), 0.39972538, 0.15367131),\n",
       " (array([ 1.]), -0.095560297, 0.97044563)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(β, w.eval(), qw.eval()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.0, array([-0.54414594], dtype=float32), array([-0.9823342], dtype=float32))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " α, b.eval(), qb.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More examples\n",
    "\n",
    "See [tutorials](http://edwardlib.org/tutorials/)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
